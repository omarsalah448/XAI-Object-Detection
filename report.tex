\documentclass{article}
\usepackage{graphicx} % Required for inserting images
\usepackage{float}

\title{Assignment4}
\author{Youssef Yosry (6953), Omar Salah (6809), Ali Mohamed Ali (6835)}
\date{1st January 2024}

\begin{document}

\maketitle

\section{Faster RCNN}

Faster R-CNN was proposed in 2015 as an object Detection model with Region Proposal Networks that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. It is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. It is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. \cite{ren2016faster}


\begin{figure}[H]
  \centering
  \includegraphics[width=0.7\textwidth]{Screenshot 2024-01-01 194548.png} % Replace 'example-image' with the filename of your image
  \caption{Faster RCNN Figure}
  \label{fig:example}
\end{figure}


\begin{enumerate}
    \item \textbf{Convolution layers}
    
    Convolution layers play a crucial role in feature extraction from input images. These layers employ convolutional operations to capture hierarchical features, enabling the model to understand spatial patterns.
    
    \item \textbf{Region Proposal Network (RPN)}
    
    A Region Proposal Network (RPN) takes an image (of any size) as input and produces a set of rectangular object proposals, each associated with an objectness score. This is achieved through a fully convolutional network that slides over the convolutional feature map generated by shared layers. The mini-network within the sliding window generates lower-dimensional features, which are then processed by sibling fully connected layers for box regression and box classification.
    
    \item \textbf{Classes and Bounding Boxes Prediction}
    
    To predict classes and bounding boxes, the model utilizes the shared convolutional layers and the region proposals generated by the RPN. The final predictions involve a combination of classification and regression tasks. The model's architecture is designed to efficiently share computation between the RPN and the subsequent object detection network.
\end{enumerate}

A Region Proposal Network (RPN) takes an image (of any size) as input and outputs a set of rectangular object proposals, each with an objectness score. This process was modeled with a fully convolutional network. Because the goal is to share computation with a Fast R-CNN object detection network, it was assumed that both nets share a common set of convolutional layers. To generate region proposals, we slide a small network over the convolutional feature map output by the last shared convolutional layer. This small network takes as input an $n \times n$ spatial window of the input convolutional feature map. Each sliding window is mapped to a lower-dimensional feature. This feature is fed into two sibling fully connected layers—a box-regression layer (\textit{reg}) and a box-classification layer \cite{carion2020endtoend}...  (\textit{cls}).

Note that because the mini-network operates in a sliding-window fashion, the fully-connected layers are shared across all spatial locations. This architecture is naturally implemented with an $n \times n$ convolutional layer followed by two sibling $1 \times 1$ convolutional layers (for \textit{reg} and \textit{cls}, respectively).


\begin{figure}[H]
  \centering
  \includegraphics[width=0.7\textwidth]{Screenshot 2024-01-01 195551.png} % Replace 'example-image' with the filename of your image
  \caption{Model Architecture}
  \label{fig:example}
\end{figure}



At each sliding-window location, multiple region proposals are predicted simultaneously, where the number of maximum possible proposals for each location is denoted as \(k\). The \texttt{reg} layer has \(4k\) outputs encoding the coordinates of \(k\) boxes, and the \texttt{cls} layer outputs \(2k\) scores that estimate the probability of an object or not an object for each proposal. The \(k\) proposals are parameterized relative to \(k\) reference boxes, which are called anchors. An anchor is centered at the sliding window in question and is associated with a scale and aspect ratio.

\textbf{Pros:}
\begin{itemize}
  \item \textbf{Accuracy:} Faster R-CNN is one of the most accurate object detection algorithms.
  \item \textbf{Unified Network:} It appears to the user as a single, end-to-end, unified network.
  \item \textbf{Region Proposal Network (RPN):} Faster R-CNN builds a region-proposal network that can generate region proposals, which are then fed to the detection model.
  \item \textbf{Feature Sharing:} It shares features between the RPN and Fast R-CNN, making it more efficient.
\end{itemize}

\textbf{Cons:}
\begin{enumerate}
  \item \textbf{Inference Time:} It requires a lot of power at inference time.
  \item \textbf{Data Requirements:} These models need large amounts of annotated data to train and fine-tune, which can be costly, time-consuming, and prone to errors.
  \item \textbf{Computational Complexity:} Faster R-CNN can be computationally expensive, especially during training. The Region Proposal Network adds complexity to the overall architecture.
  \item \textbf{Not Real-time for All Applications:} Although faster than many other object detection models, Faster R-CNN might not be suitable for real-time applications with strict latency requirements.
\end{enumerate}


\section{YOLO V5}

\begin{enumerate}
    \item \textbf{Backbone:} The main body of YOLOv5 employs the New CSP-Darknet53 structure, a modification of the Darknet architecture used in previous versions.
    
    \item \textbf{Neck:} Connecting the backbone and the head, the neck in YOLOv5 utilizes SPPF and New CSP-PAN structures.
    
    \item \textbf{Head:} Responsible for generating the final output, YOLOv5 uses the YOLOv3 Head for this purpose.
\end{enumerate}

\section{Pros}

\begin{enumerate}
    \item \textbf{Speed:} YOLOv5 is recognized for its fast inference time, making it suitable for real-time object detection.
    
    \item \textbf{Ease of Use:} YOLOv5 is user-friendly compared to some other models.
    
    \item \textbf{Variants:} YOLOv5 comes in four main versions: small (s), medium (m), large (l), and extra-large (x), each offering progressively higher accuracy rates.
    
    \item \textbf{Performance:} Preliminary results indicate strong performance relative to other state-of-the-art techniques.
\end{enumerate}

\section{Cons}

\begin{enumerate}
    \item \textbf{Small Object Detection:} Detecting small objects in images can be challenging.
    
    \item \textbf{Object Proximity:} Objects very close to each other may be challenging to detect due to the grid structure.
    
    \item \textbf{Localization Error:} YOLOv5 exhibits comparatively higher localization and low recall errors compared to Faster R-CNN.

\end{enumerate}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.7\textwidth]{Screenshot 2024-01-01 200120.png} % Replace 'example-image' with the filename of your image
  \caption{Model Architecture}
  \label{fig:example}
\end{figure}

\section{DETR}

The main ingredients of the new framework, called DEtection TRansformer or DETR, consist of a set-based global loss that enforces unique predictions through bi-partite matching and a transformer encoder-decoder architecture. DETR utilizes a fixed small set of learned object queries to reason about the relations of the objects and the global image context, directly producing the final set of predictions in parallel.

The overall DETR architecture is simple, comprising three main components:

\begin{enumerate}
    \item A CNN backbone to extract a compact feature representation.
    
    \item An encoder-decoder transformer.
    
    \item A simple feed-forward network (FFN) that makes the final detection prediction.
\end{enumerate}


\begin{figure}[H]
  \centering
  \includegraphics[width=0.7\textwidth]{Screenshot 2024-01-01 200603.png} % Replace 'example-image' with the filename of your image
  \caption{Model Architecture}
  \label{fig:example}
\end{figure}

\textbf{Pros:}

\begin{enumerate}
    \item Simplicity: DETR has a simple architecture compared to other techniques, which often have many hyperparameters and layers.
    
    \item End-to-End Detector: DETR views object detection as a set prediction problem and introduces a one-to-one set matching scheme based on a transformer encoder-decoder architecture.
    
    \item No Need for NMS: DETR does not require hand-crafted non-maximum suppression (NMS).
    
    \item Performance on Large Objects: DETR demonstrates significantly better performance on large objects.
    
    \item Global Context: The transformer architecture in DETR allows it to capture global context information, enabling better understanding of the relationships between objects in an image.
    
    \item Panoptic Segmentation: DETR can be extended to perform panoptic segmentation, which combines semantic segmentation and instance segmentation into a single framework. This can be advantageous for tasks that require a comprehensive understanding of the scene.
\end{enumerate}

\textbf{Cons:}

\begin{enumerate}
    \item Performance on Small Objects: DETR does not perform as well on small objects.
    
    \item Quadratic Complexity: The Transformer encoder in DETR has quadratic complexity.
    
    \item Sparse Supervision: Too few queries assigned as positive samples in DETR with one-to-one set matching leads to sparse supervision.
\end{enumerate}

\section{Results}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.7\textwidth]{Screenshot 2024-01-01 200838.png} % Replace 'example-image' with the filename of your image
  \caption{Results}
  \label{fig:example}
\end{figure}

\section{XAI Methods}

\textbf{Methods that worked with Faster R-CNN:}
\begin{itemize}
    \item Eigencam
    \item Ablationcam
    \item Scorecam
    \item Hirescam
    \item Xgradcam
\end{itemize}

\textbf{Yolo:}
\begin{itemize}
    \item Eigencam
\end{itemize}

\textbf{DETR:}
\begin{itemize}
    \item Attention map
\end{itemize}


\section{Faster R-CNN Results}


Faster R-CNN excels in object detection with a robust combination of a Region Proposal Network (RPN) and a Fast R-CNN detection network. Its speed and accuracy make it a top choice for real-time applications across diverse object classes.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.7\textwidth]{Screenshot 2024-01-01 200959.png} % Replace 'example-image' with the filename of your image
  \caption{Faster R-CNN output}
  \label{fig:example}
\end{figure}


\begin{figure}[H]
  \centering
  \includegraphics[width=0.7\textwidth]{Screenshot 2024-01-01 201405.png} % Replace 'example-image' with the filename of your image
  \caption{Faster R-CNN output}
  \label{fig:example}
\end{figure}



% \begin{figure}[h]
%   \centering
%   \includegraphics[width=0.7\textwidth]{Screenshot 2024-01-01 201405.png} % Replace 'example-image' with the filename of your image
%   \caption{Example Image}
%   \label{fig:example}
% \end{figure}


\begin{figure}[H]
  \centering
  \includegraphics[width=0.7\textwidth]{Screenshot 2024-01-01 201517.png} % Replace 'example-image' with the filename of your image
  \caption{Faster R-CNN output}
  \label{fig:example}
\end{figure}



\section{YOLO results}

YOLO demonstrates swift and accurate object detection, particularly notable for its real-time capabilities. With a unified architecture, variants ranging from small to extra-large offer flexibility to meet specific accuracy requirements. Despite challenges in detecting small objects and handling object proximity, YOLO's speed and ease of use make it a popular choice for various applications.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.7\textwidth]{Screenshot 2024-01-01 201635.png} % Replace 'example-image' with the filename of your image
  \caption{YOLO output}
  \label{fig:example}
\end{figure}


\begin{figure}[H]
  \centering
  \includegraphics[width=0.7\textwidth]{Screenshot 2024-01-01 201722.png} % Replace 'example-image' with the filename of your image
  \caption{YOLO output}
  \label{fig:example}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.7\textwidth]{Screenshot 2024-01-01 201949.png} % Replace 'example-image' with the filename of your image
  \caption{YOLO output}
  \label{fig:example}
\end{figure}


\begin{figure}[H]
  \centering
  \includegraphics[width=0.7\textwidth]{Screenshot 2024-01-01 202104.png} % Replace 'example-image' with the filename of your image
  \caption{YOLO output}
  \label{fig:example}
\end{figure}


\begin{figure}[H]
  \centering
  \includegraphics[width=0.7\textwidth]{Screenshot 2024-01-01 202147.png} % Replace 'example-image' with the filename of your image
  \caption{YOLO output}
  \label{fig:example}
\end{figure}


\begin{figure}[H]
  \centering
  \includegraphics[width=0.7\textwidth]{Screenshot 2024-01-01 202229.png} % Replace 'example-image' with the filename of your image
  \caption{YOLO output}
  \label{fig:example}
\end{figure}

\section{DETR results}


DETR, or the Detection Transformer, offers simplicity in its architecture compared to other techniques, featuring a transformer encoder-decoder setup. Leveraging set-based global loss and a transformer framework, DETR directly outputs predictions for a fixed set of learned object queries. While excelling in tasks related to large objects and global context understanding, DETR faces challenges in small object detection, quadratic complexity in its transformer encoder, and sparse supervision.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.7\textwidth]{Screenshot 2024-01-01 202450.png} % Replace 'example-image' with the filename of your image
  \caption{DETR output}
  \label{fig:example}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.7\textwidth]{Screenshot 2024-01-01 202553.png} % Replace 'example-image' with the filename of your image
  \caption{DETR output}
  \label{fig:example}
\end{figure}

\section{PASCAL Dataset}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.7\textwidth]{Screenshot 2024-01-01 202756.png} % Replace 'example-image' with the filename of your image
  \caption{Example Image}
  \label{fig:example}
\end{figure}

From this table we can observe that the results are VERY different from the COCO dataset. After some inspection we found that the reason is that the COCO dataset’s bounding boxes are very bad but the models output the correct bounding boxes.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.7\textwidth]{Screenshot 2024-01-01 202856.png} % Replace 'example-image' with the filename of your image
  \caption{True Boxes from the dataset}
  \label{fig:example}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.7\textwidth]{Screenshot 2024-01-01 202955.png} % Replace 'example-image' with the filename of your image
  \caption{Predicted boxes from Faster R-CNN}
  \label{fig:example}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.7\textwidth]{Screenshot 2024-01-01 203050.png} % Replace 'example-image' with the filename of your image
  \caption{Predicted boxes from YO-LO}
  \label{fig:example}
\end{figure}

\section{Images from PASCAL Data-set}

In order to assess and compare the performance of the three models, namely DETR, YOLO, and Faster R-CNN, we conducted an evaluation on a distinct dataset, specifically the PASCAL VOC 2012. This dataset provides a different set of challenges compared to COCO, allowing us to gauge the models' generalization and robustness across diverse datasets. Here, we present sample outputs from each model on the PASCAL VOC 2012 dataset to offer insights into their detection capabilities in this alternative context.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.7\textwidth]{Screenshot 2024-01-01 203244.png} % Replace 'example-image' with the filename of your image
  \caption{Faster R-CNN}
  \label{fig:example}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.7\textwidth]{Screenshot 2024-01-01 203605.png} % Replace 'example-image' with the filename of your image
  \caption{YOLO}
  \label{fig:example}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.7\textwidth]{Screenshot 2024-01-01 203659.png} % Replace 'example-image' with the filename of your image
  \caption{DETR}
  \label{fig:example}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.7\textwidth]{Screenshot 2024-01-01 203808.png} % Replace 'example-image' with the filename of your image
  \caption{DETR}
  \label{fig:example}
\end{figure}




% Rest of your document content

\bibliographystyle{plain}
\bibliography{refrences}





\end{document}
